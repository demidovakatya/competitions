{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.kaggle.com/zfturbo/talkingdata-mobile-user-demographics/xgboost-simple-starter/code\n",
    "\n",
    "```python\n",
    "__author__ = 'ZFTurbo: https://kaggle.com/zfturbo'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import zipfile\n",
    "import time\n",
    "import shutil\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(2016)\n",
    "\n",
    "def run_xgb(train, test, features, target, random_state=0):\n",
    "    eta = 0.1\n",
    "    max_depth = 3\n",
    "    subsample = 0.7\n",
    "    colsample_bytree = 0.7\n",
    "    start_time = time.time()\n",
    "\n",
    "    print('XGBoost params. ETA: {}, MAX_DEPTH: {}, SUBSAMPLE: {}, COLSAMPLE_BY_TREE: {}'.format(eta, max_depth, subsample, colsample_bytree))\n",
    "    params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": 12,\n",
    "        \"booster\" : \"gbtree\",\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"eta\": eta,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"subsample\": subsample,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"silent\": 1,\n",
    "        \"seed\": random_state,\n",
    "    }\n",
    "    num_boost_round = 500\n",
    "    early_stopping_rounds = 50\n",
    "    test_size = 0.3\n",
    "\n",
    "    X_train, X_valid = train_test_split(train, test_size=test_size, random_state=random_state)\n",
    "    print('Length train:', len(X_train.index))\n",
    "    print('Length valid:', len(X_valid.index))\n",
    "    y_train = X_train[target]\n",
    "    y_valid = X_valid[target]\n",
    "    dtrain = xgb.DMatrix(X_train[features], y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid[features], y_valid)\n",
    "\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n",
    "\n",
    "    print(\"Validating...\")\n",
    "    check = gbm.predict(xgb.DMatrix(X_valid[features]), ntree_limit=gbm.best_iteration)\n",
    "    score = log_loss(y_valid.tolist(), check)\n",
    "\n",
    "    print(\"Predict test set...\")\n",
    "    test_prediction = gbm.predict(xgb.DMatrix(test[features]), ntree_limit=gbm.best_iteration)\n",
    "\n",
    "    print('Training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "    return test_prediction.tolist(), score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_submission(score, test, prediction):\n",
    "    # Make Submission\n",
    "    now = datetime.datetime.now()\n",
    "    sub_file = 'submission_' + str(score) + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "    print('Writing submission: ', sub_file)\n",
    "    f = open(sub_file, 'w')\n",
    "    f.write('device_id,F23-,F24-26,F27-28,F29-32,F33-42,F43+,M22-,M23-26,M27-28,M29-31,M32-38,M39+\\n')\n",
    "    total = 0\n",
    "    test_val = test['device_id'].values\n",
    "    for i in range(len(test_val)):\n",
    "        str1 = str(test_val[i])\n",
    "        for j in range(12):\n",
    "            str1 += ',' + str(prediction[i][j])\n",
    "        str1 += '\\n'\n",
    "        total += 1\n",
    "        f.write(str1)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_column(table, f):\n",
    "    labels = sorted(table[f].unique())\n",
    "    mappings = dict()\n",
    "    for i in range(len(labels)):\n",
    "        mappings[labels[i]] = i\n",
    "    table = table.replace({f: mappings})\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_train_test():\n",
    "    # Events\n",
    "    print('Read events...')\n",
    "    events = pd.read_csv(\"data/events.csv\", dtype={'device_id': np.str})\n",
    "    events['counts'] = events.groupby(['device_id'])['event_id'].transform('count')\n",
    "    events_small = events[['device_id', 'counts']].drop_duplicates('device_id', keep='first')\n",
    "\n",
    "    # Phone brand\n",
    "    print('Read brands...')\n",
    "    pbd = pd.read_csv(\"data/phone_brand_device_model.csv\", dtype={'device_id': np.str})\n",
    "    pbd.drop_duplicates('device_id', keep='first', inplace=True)\n",
    "    pbd = map_column(pbd, 'phone_brand')\n",
    "    pbd = map_column(pbd, 'device_model')\n",
    "\n",
    "    # Train\n",
    "    print('Read train...')\n",
    "    train = pd.read_csv(\"data/gender_age_train.csv\", dtype={'device_id': np.str})\n",
    "    train = map_column(train, 'group')\n",
    "    train = train.drop(['age'], axis=1)\n",
    "    train = train.drop(['gender'], axis=1)\n",
    "    train = pd.merge(train, pbd, how='left', on='device_id', left_index=True)\n",
    "    train = pd.merge(train, events_small, how='left', on='device_id', left_index=True)\n",
    "    train.fillna(-1, inplace=True)\n",
    "\n",
    "    # Test\n",
    "    print('Read test...')\n",
    "    test = pd.read_csv(\"data/gender_age_test.csv\", dtype={'device_id': np.str})\n",
    "    test = pd.merge(test, pbd, how='left', on='device_id', left_index=True)\n",
    "    test = pd.merge(test, events_small, how='left', on='device_id', left_index=True)\n",
    "    test.fillna(-1, inplace=True)\n",
    "\n",
    "    # Features\n",
    "    features = list(test.columns.values)\n",
    "    features.remove('device_id')\n",
    "\n",
    "    return train, test, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read events...\n",
      "Read brands...\n",
      "Read train...\n",
      "Read test...\n"
     ]
    }
   ],
   "source": [
    "train, test, features = read_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train:  74645\n",
      "Length of test:  112071\n",
      "Features [3]: ['counts', 'device_model', 'phone_brand']\n"
     ]
    }
   ],
   "source": [
    "print('Length of train: ', len(train))\n",
    "print('Length of test: ', len(test))\n",
    "print('Features [{}]: {}'.format(len(features), sorted(features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost params. ETA: 0.1, MAX_DEPTH: 3, SUBSAMPLE: 0.7, COLSAMPLE_BY_TREE: 0.7\n",
      "Length train: 52251\n",
      "Length valid: 22394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "[0]\ttrain-mlogloss:2.474397\teval-mlogloss:2.474914\n",
      "[1]\ttrain-mlogloss:2.465317\teval-mlogloss:2.466109\n",
      "[2]\ttrain-mlogloss:2.457118\teval-mlogloss:2.458275\n",
      "[3]\ttrain-mlogloss:2.450113\teval-mlogloss:2.451398\n",
      "[4]\ttrain-mlogloss:2.443151\teval-mlogloss:2.445095\n",
      "[5]\ttrain-mlogloss:2.437325\teval-mlogloss:2.439628\n",
      "[6]\ttrain-mlogloss:2.432078\teval-mlogloss:2.434799\n",
      "[7]\ttrain-mlogloss:2.427336\teval-mlogloss:2.430429\n",
      "[8]\ttrain-mlogloss:2.423183\teval-mlogloss:2.426577\n",
      "[9]\ttrain-mlogloss:2.419263\teval-mlogloss:2.423142\n",
      "[10]\ttrain-mlogloss:2.415664\teval-mlogloss:2.419885\n",
      "[11]\ttrain-mlogloss:2.412357\teval-mlogloss:2.416944\n",
      "[12]\ttrain-mlogloss:2.409511\teval-mlogloss:2.414311\n",
      "[13]\ttrain-mlogloss:2.406846\teval-mlogloss:2.412080\n",
      "[14]\ttrain-mlogloss:2.404468\teval-mlogloss:2.410016\n",
      "[15]\ttrain-mlogloss:2.402271\teval-mlogloss:2.408044\n",
      "[16]\ttrain-mlogloss:2.400320\teval-mlogloss:2.406388\n",
      "[17]\ttrain-mlogloss:2.398505\teval-mlogloss:2.404837\n",
      "[18]\ttrain-mlogloss:2.396853\teval-mlogloss:2.403495\n",
      "[19]\ttrain-mlogloss:2.395257\teval-mlogloss:2.402129\n",
      "[20]\ttrain-mlogloss:2.393752\teval-mlogloss:2.400977\n",
      "[21]\ttrain-mlogloss:2.392480\teval-mlogloss:2.399953\n",
      "[22]\ttrain-mlogloss:2.391119\teval-mlogloss:2.398949\n",
      "[23]\ttrain-mlogloss:2.390052\teval-mlogloss:2.398129\n",
      "[24]\ttrain-mlogloss:2.388925\teval-mlogloss:2.397420\n",
      "[25]\ttrain-mlogloss:2.387869\teval-mlogloss:2.396662\n",
      "[26]\ttrain-mlogloss:2.386950\teval-mlogloss:2.396100\n",
      "[27]\ttrain-mlogloss:2.386137\teval-mlogloss:2.395540\n",
      "[28]\ttrain-mlogloss:2.385370\teval-mlogloss:2.395092\n",
      "[29]\ttrain-mlogloss:2.384625\teval-mlogloss:2.394578\n",
      "[30]\ttrain-mlogloss:2.383942\teval-mlogloss:2.394168\n",
      "[31]\ttrain-mlogloss:2.383244\teval-mlogloss:2.393710\n",
      "[32]\ttrain-mlogloss:2.382668\teval-mlogloss:2.393399\n",
      "[33]\ttrain-mlogloss:2.382133\teval-mlogloss:2.393106\n",
      "[34]\ttrain-mlogloss:2.381594\teval-mlogloss:2.392864\n",
      "[35]\ttrain-mlogloss:2.381004\teval-mlogloss:2.392507\n",
      "[36]\ttrain-mlogloss:2.380434\teval-mlogloss:2.392194\n",
      "[37]\ttrain-mlogloss:2.379920\teval-mlogloss:2.391915\n",
      "[38]\ttrain-mlogloss:2.379472\teval-mlogloss:2.391721\n",
      "[39]\ttrain-mlogloss:2.379052\teval-mlogloss:2.391581\n",
      "[40]\ttrain-mlogloss:2.378654\teval-mlogloss:2.391412\n",
      "[41]\ttrain-mlogloss:2.378169\teval-mlogloss:2.391198\n",
      "[42]\ttrain-mlogloss:2.377830\teval-mlogloss:2.391075\n",
      "[43]\ttrain-mlogloss:2.377261\teval-mlogloss:2.390846\n",
      "[44]\ttrain-mlogloss:2.376931\teval-mlogloss:2.390729\n",
      "[45]\ttrain-mlogloss:2.376591\teval-mlogloss:2.390633\n",
      "[46]\ttrain-mlogloss:2.376205\teval-mlogloss:2.390502\n",
      "[47]\ttrain-mlogloss:2.375793\teval-mlogloss:2.390373\n",
      "[48]\ttrain-mlogloss:2.375498\teval-mlogloss:2.390266\n",
      "[49]\ttrain-mlogloss:2.375127\teval-mlogloss:2.390071\n",
      "[50]\ttrain-mlogloss:2.374799\teval-mlogloss:2.389972\n",
      "[51]\ttrain-mlogloss:2.374492\teval-mlogloss:2.389871\n",
      "[52]\ttrain-mlogloss:2.374247\teval-mlogloss:2.389719\n",
      "[53]\ttrain-mlogloss:2.373918\teval-mlogloss:2.389672\n",
      "[54]\ttrain-mlogloss:2.373595\teval-mlogloss:2.389622\n",
      "[55]\ttrain-mlogloss:2.373344\teval-mlogloss:2.389603\n",
      "[56]\ttrain-mlogloss:2.373025\teval-mlogloss:2.389450\n",
      "[57]\ttrain-mlogloss:2.372806\teval-mlogloss:2.389448\n",
      "[58]\ttrain-mlogloss:2.372554\teval-mlogloss:2.389394\n",
      "[59]\ttrain-mlogloss:2.372310\teval-mlogloss:2.389403\n",
      "[60]\ttrain-mlogloss:2.372123\teval-mlogloss:2.389404\n",
      "[61]\ttrain-mlogloss:2.371820\teval-mlogloss:2.389363\n",
      "[62]\ttrain-mlogloss:2.371534\teval-mlogloss:2.389283\n",
      "[63]\ttrain-mlogloss:2.371297\teval-mlogloss:2.389223\n",
      "[64]\ttrain-mlogloss:2.371024\teval-mlogloss:2.389183\n",
      "[65]\ttrain-mlogloss:2.370769\teval-mlogloss:2.389124\n",
      "[66]\ttrain-mlogloss:2.370510\teval-mlogloss:2.389058\n",
      "[67]\ttrain-mlogloss:2.370290\teval-mlogloss:2.388976\n",
      "[68]\ttrain-mlogloss:2.370078\teval-mlogloss:2.388934\n",
      "[69]\ttrain-mlogloss:2.369910\teval-mlogloss:2.388949\n",
      "[70]\ttrain-mlogloss:2.369627\teval-mlogloss:2.388868\n",
      "[71]\ttrain-mlogloss:2.369365\teval-mlogloss:2.388865\n",
      "[72]\ttrain-mlogloss:2.369144\teval-mlogloss:2.388802\n",
      "[73]\ttrain-mlogloss:2.368930\teval-mlogloss:2.388791\n",
      "[74]\ttrain-mlogloss:2.368578\teval-mlogloss:2.388795\n",
      "[75]\ttrain-mlogloss:2.368398\teval-mlogloss:2.388749\n",
      "[76]\ttrain-mlogloss:2.368043\teval-mlogloss:2.388674\n",
      "[77]\ttrain-mlogloss:2.367796\teval-mlogloss:2.388633\n",
      "[78]\ttrain-mlogloss:2.367534\teval-mlogloss:2.388518\n",
      "[79]\ttrain-mlogloss:2.367347\teval-mlogloss:2.388568\n",
      "[80]\ttrain-mlogloss:2.367105\teval-mlogloss:2.388537\n",
      "[81]\ttrain-mlogloss:2.366878\teval-mlogloss:2.388436\n",
      "[82]\ttrain-mlogloss:2.366664\teval-mlogloss:2.388452\n",
      "[83]\ttrain-mlogloss:2.366484\teval-mlogloss:2.388403\n",
      "[84]\ttrain-mlogloss:2.366208\teval-mlogloss:2.388395\n",
      "[85]\ttrain-mlogloss:2.365998\teval-mlogloss:2.388345\n",
      "[86]\ttrain-mlogloss:2.365729\teval-mlogloss:2.388266\n",
      "[87]\ttrain-mlogloss:2.365487\teval-mlogloss:2.388309\n",
      "[88]\ttrain-mlogloss:2.365274\teval-mlogloss:2.388255\n",
      "[89]\ttrain-mlogloss:2.365010\teval-mlogloss:2.388217\n",
      "[90]\ttrain-mlogloss:2.364834\teval-mlogloss:2.388205\n",
      "[91]\ttrain-mlogloss:2.364640\teval-mlogloss:2.388169\n",
      "[92]\ttrain-mlogloss:2.364451\teval-mlogloss:2.388182\n",
      "[93]\ttrain-mlogloss:2.364241\teval-mlogloss:2.388144\n",
      "[94]\ttrain-mlogloss:2.364041\teval-mlogloss:2.388108\n",
      "[95]\ttrain-mlogloss:2.363852\teval-mlogloss:2.388119\n",
      "[96]\ttrain-mlogloss:2.363643\teval-mlogloss:2.388065\n",
      "[97]\ttrain-mlogloss:2.363410\teval-mlogloss:2.388015\n",
      "[98]\ttrain-mlogloss:2.363210\teval-mlogloss:2.388030\n",
      "[99]\ttrain-mlogloss:2.363008\teval-mlogloss:2.388062\n",
      "[100]\ttrain-mlogloss:2.362876\teval-mlogloss:2.388084\n",
      "[101]\ttrain-mlogloss:2.362656\teval-mlogloss:2.388073\n",
      "[102]\ttrain-mlogloss:2.362393\teval-mlogloss:2.387917\n",
      "[103]\ttrain-mlogloss:2.362197\teval-mlogloss:2.387943\n",
      "[104]\ttrain-mlogloss:2.362045\teval-mlogloss:2.387949\n",
      "[105]\ttrain-mlogloss:2.361840\teval-mlogloss:2.387855\n",
      "[106]\ttrain-mlogloss:2.361661\teval-mlogloss:2.387874\n",
      "[107]\ttrain-mlogloss:2.361512\teval-mlogloss:2.387880\n",
      "[108]\ttrain-mlogloss:2.361226\teval-mlogloss:2.387856\n",
      "[109]\ttrain-mlogloss:2.361048\teval-mlogloss:2.387882\n",
      "[110]\ttrain-mlogloss:2.360846\teval-mlogloss:2.387844\n",
      "[111]\ttrain-mlogloss:2.360624\teval-mlogloss:2.387848\n",
      "[112]\ttrain-mlogloss:2.360490\teval-mlogloss:2.387835\n",
      "[113]\ttrain-mlogloss:2.360342\teval-mlogloss:2.387846\n",
      "[114]\ttrain-mlogloss:2.360115\teval-mlogloss:2.387861\n",
      "[115]\ttrain-mlogloss:2.359951\teval-mlogloss:2.387824\n",
      "[116]\ttrain-mlogloss:2.359739\teval-mlogloss:2.387793\n",
      "[117]\ttrain-mlogloss:2.359644\teval-mlogloss:2.387830\n",
      "[118]\ttrain-mlogloss:2.359402\teval-mlogloss:2.387769\n",
      "[119]\ttrain-mlogloss:2.359239\teval-mlogloss:2.387767\n",
      "[120]\ttrain-mlogloss:2.359087\teval-mlogloss:2.387741\n",
      "[121]\ttrain-mlogloss:2.358943\teval-mlogloss:2.387708\n",
      "[122]\ttrain-mlogloss:2.358756\teval-mlogloss:2.387701\n",
      "[123]\ttrain-mlogloss:2.358607\teval-mlogloss:2.387693\n",
      "[124]\ttrain-mlogloss:2.358419\teval-mlogloss:2.387715\n",
      "[125]\ttrain-mlogloss:2.358267\teval-mlogloss:2.387655\n",
      "[126]\ttrain-mlogloss:2.358073\teval-mlogloss:2.387591\n",
      "[127]\ttrain-mlogloss:2.357861\teval-mlogloss:2.387625\n",
      "[128]\ttrain-mlogloss:2.357687\teval-mlogloss:2.387614\n",
      "[129]\ttrain-mlogloss:2.357510\teval-mlogloss:2.387588\n",
      "[130]\ttrain-mlogloss:2.357364\teval-mlogloss:2.387547\n",
      "[131]\ttrain-mlogloss:2.357187\teval-mlogloss:2.387560\n",
      "[132]\ttrain-mlogloss:2.357029\teval-mlogloss:2.387564\n",
      "[133]\ttrain-mlogloss:2.356851\teval-mlogloss:2.387550\n",
      "[134]\ttrain-mlogloss:2.356730\teval-mlogloss:2.387614\n",
      "[135]\ttrain-mlogloss:2.356557\teval-mlogloss:2.387620\n",
      "[136]\ttrain-mlogloss:2.356369\teval-mlogloss:2.387555\n",
      "[137]\ttrain-mlogloss:2.356183\teval-mlogloss:2.387478\n",
      "[138]\ttrain-mlogloss:2.356018\teval-mlogloss:2.387485\n",
      "[139]\ttrain-mlogloss:2.355811\teval-mlogloss:2.387443\n",
      "[140]\ttrain-mlogloss:2.355616\teval-mlogloss:2.387435\n",
      "[141]\ttrain-mlogloss:2.355450\teval-mlogloss:2.387467\n",
      "[142]\ttrain-mlogloss:2.355287\teval-mlogloss:2.387466\n",
      "[143]\ttrain-mlogloss:2.355122\teval-mlogloss:2.387424\n",
      "[144]\ttrain-mlogloss:2.354906\teval-mlogloss:2.387473\n",
      "[145]\ttrain-mlogloss:2.354739\teval-mlogloss:2.387497\n",
      "[146]\ttrain-mlogloss:2.354590\teval-mlogloss:2.387442\n",
      "[147]\ttrain-mlogloss:2.354450\teval-mlogloss:2.387427\n",
      "[148]\ttrain-mlogloss:2.354254\teval-mlogloss:2.387403\n",
      "[149]\ttrain-mlogloss:2.354124\teval-mlogloss:2.387453\n",
      "[150]\ttrain-mlogloss:2.353981\teval-mlogloss:2.387445\n",
      "[151]\ttrain-mlogloss:2.353832\teval-mlogloss:2.387400\n",
      "[152]\ttrain-mlogloss:2.353720\teval-mlogloss:2.387418\n",
      "[153]\ttrain-mlogloss:2.353562\teval-mlogloss:2.387458\n",
      "[154]\ttrain-mlogloss:2.353411\teval-mlogloss:2.387463\n",
      "[155]\ttrain-mlogloss:2.353289\teval-mlogloss:2.387429\n",
      "[156]\ttrain-mlogloss:2.353116\teval-mlogloss:2.387437\n",
      "[157]\ttrain-mlogloss:2.352977\teval-mlogloss:2.387412\n",
      "[158]\ttrain-mlogloss:2.352778\teval-mlogloss:2.387410\n",
      "[159]\ttrain-mlogloss:2.352592\teval-mlogloss:2.387409\n",
      "[160]\ttrain-mlogloss:2.352473\teval-mlogloss:2.387407\n",
      "[161]\ttrain-mlogloss:2.352296\teval-mlogloss:2.387387\n",
      "[162]\ttrain-mlogloss:2.352111\teval-mlogloss:2.387337\n",
      "[163]\ttrain-mlogloss:2.352006\teval-mlogloss:2.387344\n",
      "[164]\ttrain-mlogloss:2.351847\teval-mlogloss:2.387376\n",
      "[165]\ttrain-mlogloss:2.351726\teval-mlogloss:2.387363\n",
      "[166]\ttrain-mlogloss:2.351609\teval-mlogloss:2.387329\n",
      "[167]\ttrain-mlogloss:2.351447\teval-mlogloss:2.387371\n",
      "[168]\ttrain-mlogloss:2.351310\teval-mlogloss:2.387332\n",
      "[169]\ttrain-mlogloss:2.351072\teval-mlogloss:2.387341\n",
      "[170]\ttrain-mlogloss:2.350952\teval-mlogloss:2.387307\n",
      "[171]\ttrain-mlogloss:2.350760\teval-mlogloss:2.387259\n",
      "[172]\ttrain-mlogloss:2.350616\teval-mlogloss:2.387231\n",
      "[173]\ttrain-mlogloss:2.350501\teval-mlogloss:2.387217\n",
      "[174]\ttrain-mlogloss:2.350353\teval-mlogloss:2.387248\n",
      "[175]\ttrain-mlogloss:2.350223\teval-mlogloss:2.387215\n",
      "[176]\ttrain-mlogloss:2.350028\teval-mlogloss:2.387247\n",
      "[177]\ttrain-mlogloss:2.349877\teval-mlogloss:2.387212\n",
      "[178]\ttrain-mlogloss:2.349756\teval-mlogloss:2.387203\n",
      "[179]\ttrain-mlogloss:2.349662\teval-mlogloss:2.387219\n",
      "[180]\ttrain-mlogloss:2.349516\teval-mlogloss:2.387208\n",
      "[181]\ttrain-mlogloss:2.349384\teval-mlogloss:2.387269\n",
      "[182]\ttrain-mlogloss:2.349228\teval-mlogloss:2.387300\n",
      "[183]\ttrain-mlogloss:2.349100\teval-mlogloss:2.387284\n",
      "[184]\ttrain-mlogloss:2.349038\teval-mlogloss:2.387337\n",
      "[185]\ttrain-mlogloss:2.348860\teval-mlogloss:2.387346\n",
      "[186]\ttrain-mlogloss:2.348748\teval-mlogloss:2.387385\n",
      "[187]\ttrain-mlogloss:2.348634\teval-mlogloss:2.387423\n",
      "[188]\ttrain-mlogloss:2.348458\teval-mlogloss:2.387466\n",
      "[189]\ttrain-mlogloss:2.348253\teval-mlogloss:2.387399\n",
      "[190]\ttrain-mlogloss:2.348115\teval-mlogloss:2.387398\n",
      "[191]\ttrain-mlogloss:2.348053\teval-mlogloss:2.387410\n",
      "[192]\ttrain-mlogloss:2.347927\teval-mlogloss:2.387352\n",
      "[193]\ttrain-mlogloss:2.347748\teval-mlogloss:2.387398\n",
      "[194]\ttrain-mlogloss:2.347526\teval-mlogloss:2.387391\n",
      "[195]\ttrain-mlogloss:2.347438\teval-mlogloss:2.387360\n",
      "[196]\ttrain-mlogloss:2.347306\teval-mlogloss:2.387401\n",
      "[197]\ttrain-mlogloss:2.347118\teval-mlogloss:2.387369\n",
      "[198]\ttrain-mlogloss:2.346999\teval-mlogloss:2.387393\n",
      "[199]\ttrain-mlogloss:2.346854\teval-mlogloss:2.387427\n",
      "[200]\ttrain-mlogloss:2.346758\teval-mlogloss:2.387449\n",
      "[201]\ttrain-mlogloss:2.346615\teval-mlogloss:2.387469\n",
      "[202]\ttrain-mlogloss:2.346448\teval-mlogloss:2.387457\n",
      "[203]\ttrain-mlogloss:2.346355\teval-mlogloss:2.387497\n",
      "[204]\ttrain-mlogloss:2.346187\teval-mlogloss:2.387541\n",
      "[205]\ttrain-mlogloss:2.346081\teval-mlogloss:2.387514\n",
      "[206]\ttrain-mlogloss:2.345948\teval-mlogloss:2.387534\n",
      "[207]\ttrain-mlogloss:2.345834\teval-mlogloss:2.387548\n",
      "[208]\ttrain-mlogloss:2.345702\teval-mlogloss:2.387539\n",
      "[209]\ttrain-mlogloss:2.345533\teval-mlogloss:2.387535\n",
      "[210]\ttrain-mlogloss:2.345393\teval-mlogloss:2.387531\n",
      "[211]\ttrain-mlogloss:2.345234\teval-mlogloss:2.387502\n",
      "[212]\ttrain-mlogloss:2.345146\teval-mlogloss:2.387476\n",
      "[213]\ttrain-mlogloss:2.344975\teval-mlogloss:2.387471\n",
      "[214]\ttrain-mlogloss:2.344861\teval-mlogloss:2.387469\n",
      "[215]\ttrain-mlogloss:2.344728\teval-mlogloss:2.387500\n",
      "[216]\ttrain-mlogloss:2.344543\teval-mlogloss:2.387505\n",
      "[217]\ttrain-mlogloss:2.344449\teval-mlogloss:2.387519\n",
      "[218]\ttrain-mlogloss:2.344326\teval-mlogloss:2.387522\n",
      "[219]\ttrain-mlogloss:2.344188\teval-mlogloss:2.387517\n",
      "[220]\ttrain-mlogloss:2.344051\teval-mlogloss:2.387554\n",
      "[221]\ttrain-mlogloss:2.343922\teval-mlogloss:2.387485\n",
      "[222]\ttrain-mlogloss:2.343780\teval-mlogloss:2.387501\n",
      "[223]\ttrain-mlogloss:2.343649\teval-mlogloss:2.387507\n",
      "[224]\ttrain-mlogloss:2.343476\teval-mlogloss:2.387507\n",
      "[225]\ttrain-mlogloss:2.343310\teval-mlogloss:2.387474\n",
      "[226]\ttrain-mlogloss:2.343164\teval-mlogloss:2.387486\n",
      "[227]\ttrain-mlogloss:2.343091\teval-mlogloss:2.387493\n",
      "[228]\ttrain-mlogloss:2.342924\teval-mlogloss:2.387483\n",
      "Stopping. Best iteration:\n",
      "[178]\ttrain-mlogloss:2.349756\teval-mlogloss:2.387203\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/xgboost/core.py:840: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  preds = preds.reshape(nrow, preds.size / nrow)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict test set...\n",
      "Training time: 1.11 minutes\n",
      "LS: 2.38721\n"
     ]
    }
   ],
   "source": [
    "test_prediction, score = run_xgb(train, test, features, 'group')\n",
    "print(\"LS: {}\".format(round(score, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submission:  submission_2.38720859136_2016-07-20-02-08.csv\n"
     ]
    }
   ],
   "source": [
    "create_submission(score, test, test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
